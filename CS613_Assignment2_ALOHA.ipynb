{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S7iCOCwJwzkB",
    "outputId": "37167663-93d1-47c0-e0a6-8633148c599e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\darshi doshi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: click in c:\\users\\darshi doshi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: joblib in c:\\users\\darshi doshi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\darshi doshi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in c:\\users\\darshi doshi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\darshi doshi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNIGRAM MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Perplexity Without Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average perplexity over all the sentences that are perplexable is  4619.325923758668\n",
      "the total sentences in test set are  66563\n",
      "the total no of not_perplexable sentences in the validation set are : 6387\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Here, we will be training Unigram Model on train_set_preprocessed.csv and we will return the perplexity score for test_set_preprocessed.csv\n",
    "We have already created the train_set_preprocessed.csv and test_set_preprocessed.csv in which the sentences are already tokenized taking care of the multilingual data.\n",
    "All the words in the csv files are already in their lower case and the alphanumeric characters have also been removed\n",
    "'''\n",
    "\n",
    "## CREATING THE VOCABULARY\n",
    "'''\n",
    "A whole list of unique words present in the entire corpus is created here.\n",
    "Here, the start and the end of the sentences (<s> and </s>) are excluded as they are not required in case of unigram model.\n",
    "'''\n",
    "def vocab() :\n",
    "    \n",
    "    test_set=pd.read_csv('test_set_preprocessed.csv')\n",
    "    comm = test_set['0']  ## extracting all the comments from the test set\n",
    "    \n",
    "    all_words=[]  ## creating a list of words \n",
    "    for comment in comm :\n",
    "        all_words += (str(comment).split())\n",
    "    vocabulary=list(set(all_words))  ## creating a vocab which contains all the unique words from the test set\n",
    "\n",
    "    train_set=pd.read_csv('train_set_preprocessed.csv')\n",
    "    comm=train_set['0']  ## extracting all the comments from the test set\n",
    "    \n",
    "    all_words=[]  ## creating a list of words \n",
    "    for comment in comm :\n",
    "        all_words += (str(comment).split())\n",
    "    vocabulary.extend(list(set(all_words)))  ## adding the unique words from the train set to the 'vocabulary' list\n",
    "\n",
    "    vocabulary = list(set(vocabulary))  ## ensuring that the list contains unique words after adding words from the train set\n",
    "    return all_words, vocabulary\n",
    "\n",
    "## LIST OF ALL UNIGRAMS\n",
    "'''\n",
    "This function creates and stores the dictionary in which the frequency of all the unique words are listed\n",
    "'''\n",
    "def unigram(all_words, vocabulary) :\n",
    "\n",
    "    unigram_counts={}  ## dictionary to store the count for each word\n",
    "    size=len(all_words)  ## total number of words in the whole corpus\n",
    "\n",
    "    for word in all_words :\n",
    "        if word in unigram_counts :\n",
    "            unigram_counts[word] += 1  ## Increment the frequency of the word if it already exists in the dictionary\n",
    "        else:\n",
    "            unigram_counts[word] = 1  ## First Occurence of the word\n",
    "\n",
    "    vocab_size = len(vocabulary)\n",
    "    return unigram_counts, size, vocab_size\n",
    "\n",
    "## FUNCTION TO CALCULATE THE PERPLEXITY FOR UNIGRAM MODEL\n",
    "'''\n",
    "In the unigram model, the probability of a sentence is the product of probabilities of all the words.\n",
    "In this function, we are calculating the perplexity scores of each sentence for unigram model according to the formula.\n",
    "Then the average perplexity over all the sentences in the test set is calculated and returned.\n",
    "'''\n",
    "def calc_perplexity(unigram_counts, size):\n",
    "\n",
    "    test_set=pd.read_csv('test_set_preprocessed.csv')\n",
    "    sentences=test_set['0']\n",
    "    perplexity=[]  ## list of all the unigram words which are perplexible (probability is not very low)\n",
    "    not_perplexable=[]  ## list of all the unigram words which are not perplexible (either probability = 0 or it is very low leading to infinite perplexity)\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "        probability = 1  ## initializing the value of probability for each sentence\n",
    "        words = (str(sentence).split())\n",
    "        words_in_sent = len(words)\n",
    "\n",
    "        for word in words:  ## calculating the probability of occurence of each word in a sentence\n",
    "            if word not in unigram_counts :\n",
    "                probability = 0\n",
    "            else :\n",
    "                probability *= (unigram_counts[word])/(size)\n",
    "\n",
    "        if probability != 0:\n",
    "            perplex = (1/(probability))**(1/(words_in_sent))  ## Formula to find the perplexity of a sentence\n",
    "            if perplex >= 1000000:  ## removing the sentences with very low probabilty, as then the value of perplex will become infinity\n",
    "                not_perplexable.append(sentence)\n",
    "            else :\n",
    "                perplexity.append(perplex)\n",
    "        else:\n",
    "            not_perplexable.append(sentence)\n",
    "\n",
    "    average_perplexity = sum(perplexity)/len(perplexity)  ## average perplexity over the whole test set\n",
    "    return average_perplexity, not_perplexable, perplexity\n",
    "\n",
    "## Now we will call the vocab(), the unigram(all_words, vocabulary), and the calc_perplexity(unigram_counts, size) function to get the desired output\n",
    "all_words, vocabulary = vocab()  \n",
    "unigram_counts, size, vocab_size = unigram(all_words, vocabulary)\n",
    "average_perplexity, not_perplexable, perplexity = calc_perplexity(unigram_counts, size)\n",
    "\n",
    "print(\"the average perplexity over all the sentences that are perplexable is \", average_perplexity)\n",
    "print(\"the total sentences in test set are \", len(perplexity)+ len(not_perplexable))\n",
    "print(\"the total no of not_perplexable sentences in the validation set are :\", len(not_perplexable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Perplexity after Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average perplexity over all the sentences that are perplexable is  13536.893234944624\n",
      "the total sentences in test set are  66563\n",
      "the total no of not_perplexable sentences in the validation set are : 151\n"
     ]
    }
   ],
   "source": [
    "## FUNCTION FOR CALCULATING PERPLEXITY FOR UNIGRAM MODEL AFTER LAPLACE SMOOTHING\n",
    "'''\n",
    "Laplace Smoothing helps to consider the probability of those words which are not present in the unigram_counts.\n",
    "This is achieved by adding 1 to the numerator and vocab_size to the denominator.\n",
    "In unigram model, probability of a sentence is product of probabilities of all the words\n",
    "'''\n",
    "def calc_perplexity_smoothing(unigram_counts, size, vocab_size):\n",
    "\n",
    "    test_set = pd.read_csv('test_set_preprocessed.csv')\n",
    "    sentences = test_set['0']\n",
    "    perplexity = []  ## list of all the unigram words which are perplexible (probability is not very low)\n",
    "    not_perplexable = []  ## list of all the unigram words which are not perplexible (either probability = 0 or it is very low leading to infinite perplexity)\n",
    "\n",
    "    for sentence in sentences:\n",
    "\n",
    "        probability = 1\n",
    "        perplex = 1\n",
    "        words = str(sentence).split()\n",
    "        words_in_sent=len(words)\n",
    "\n",
    "        for word in words:\n",
    "            if word not in unigram_counts:  ## Here, probability of words with 0 occurence have been taken care of\n",
    "                unigram_counts[word]=0\n",
    "            probability *= ((unigram_counts[word])+1)/(size+vocab_size)  ## Formula for probability after applying Laplace Smoothing\n",
    "\n",
    "        if probability != 0:\n",
    "            perplex = (1 / probability)**(1 / words_in_sent)  ## Formula to find the perplexity of a sentence\n",
    "            if perplex >= 100000000000:  ## removing the sentences with very low probabilty, as then the value of perplex will become infinity\n",
    "                not_perplexable.append(sentence)\n",
    "            else :\n",
    "                perplexity.append(perplex)\n",
    "        else :\n",
    "            not_perplexable.append(sentence)\n",
    "\n",
    "    average_perplexity = sum(perplexity)/len(perplexity)  ## average perplexity over the whole test set\n",
    "    return average_perplexity, not_perplexable, perplexity\n",
    "\n",
    "unigramcount_perplex, size, vocab_size = unigram(all_words, vocabulary)\n",
    "average_perplexity, not_perplexable, perplexity = calc_perplexity_smoothing(unigramcount_perplex, size, vocab_size)\n",
    "\n",
    "print(\"the average perplexity over all the sentences that are perplexable is \", average_perplexity)\n",
    "print(\"the total sentences in test set are \", len(perplexity)+ len(not_perplexable))\n",
    "print(\"the total no of not_perplexable sentences in the validation set are :\", len(not_perplexable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Perplexity after Add-k Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average perplexity over all the sentences that are perplexable is  61424.73265646102\n",
      "the total sentences in test set are  66563\n",
      "the total no of not_perplexable sentences in the validation set are : 151\n"
     ]
    }
   ],
   "source": [
    "## FUNCTION FOR CALCULATING PERPLEXITY FOR UNIGRAM MODEL AFTER ADD-k SMOOTHING\n",
    "'''\n",
    "The value of k is chosen by trial and error method.\n",
    "Generally, we should choose lower values of k starting from 0.01 or 0.1.\n",
    "Higher values of m generally > .1\n",
    "'''\n",
    "def calc_perplexity_smoothing_addk(unigram_counts, size, vocab_size):\n",
    "\n",
    "    test_set=pd.read_csv('test_set_preprocessed.csv')\n",
    "    sentences=test_set['0']\n",
    "    perplexity = []  ## list of all the unigram words which are perplexible (probability is not very low)\n",
    "    not_perplexable = []  ## list of all the unigram words which are not perplexible (either probability = 0 or it is very low leading to infinite perplexity)\n",
    "    k=0.1  ## A lower value of k is chosen\n",
    "\n",
    "    for sentence in sentences :\n",
    "\n",
    "        probability = 1\n",
    "        perplex = 1\n",
    "        words = str(sentence).split()\n",
    "        words_in_sent = len(words)\n",
    "\n",
    "        for word in words :\n",
    "            if word not in unigram_counts :\n",
    "                unigram_counts[word] = 0\n",
    "            probability *= ((unigram_counts[word])+k)/(size+(vocab_size*k))  ## Formula for probability after applying Add-k Smoothing\n",
    "\n",
    "        if probability != 0 :\n",
    "            perplex=(1/(probability))**(1/(words_in_sent))  ## Formula to find the perplexity of a sentence\n",
    "            if perplex >= 100000000000:  ## removing the sentences with very low probabilty, as then the value of perplex will become infinity\n",
    "                not_perplexable.append(sentence)\n",
    "            else :\n",
    "                perplexity.append(perplex)\n",
    "        else :\n",
    "            not_perplexable.append(sentence)\n",
    "\n",
    "    average_perplexity = sum(perplexity)/len(perplexity)\n",
    "    return average_perplexity, not_perplexable, perplexity\n",
    "\n",
    "unigramcount_perplex, size, vocab_size = unigram(all_words, vocabulary)\n",
    "\n",
    "average_perplexity, not_perplexable, perplexity = calc_perplexity_smoothing_addk(unigramcount_perplex, size, vocab_size)\n",
    "\n",
    "print(\"the average perplexity over all the sentences that are perplexable is \", average_perplexity)\n",
    "print(\"the total sentences in test set are \", len(perplexity)+ len(not_perplexable))\n",
    "print(\"the total no of not_perplexable sentences in the validation set are :\", len(not_perplexable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Perplexity after Applying Good Turing Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 8711.565898019602\n"
     ]
    }
   ],
   "source": [
    "# N_c from vocab\n",
    "def N_cfunc(unigram_counts, vocabulary):\n",
    "    Total_words = 0\n",
    "    max_c = 0\n",
    "    N = {}\n",
    "    N[0] = 0\n",
    "    for word in vocabulary:\n",
    "      if word in unigram_counts:\n",
    "        if unigram_counts[word] not in N:\n",
    "          N[unigram_counts[word]] = 1\n",
    "        else:\n",
    "          N[unigram_counts[word]] += 1\n",
    "        Total_words += unigram_counts[word]\n",
    "        max_c = max(unigram_counts[word], max_c)\n",
    "      else:\n",
    "        N[0] += 1\n",
    "\n",
    "    return N, Total_words, max_c\n",
    "\n",
    "N, Total_words, max_c = N_cfunc(unigram_counts, vocabulary)\n",
    "\n",
    "# c_star corresponding to c\n",
    "def c_star_(unigram_counts, N, max_c):\n",
    "    c_starr = {}\n",
    "\n",
    "    for c in range(max_c + 1):\n",
    "        if c in N and N[c] != 0 and (c + 1) in N and N[c + 1] != 0:\n",
    "            c_star = ((c + 1) * N[c + 1]) / N[c]\n",
    "            c_starr[c] = c_star\n",
    "\n",
    "    return c_starr\n",
    "\n",
    "c_starr = c_star_(unigram_counts, N, max_c)\n",
    "\n",
    "# N corresponding to c_star\n",
    "def N_cstar(c_starr, N):\n",
    "  N__ = {}\n",
    "  for c in N:\n",
    "    if c in c_starr:\n",
    "      N__[c_starr[c]] = N[c]\n",
    "  return N__\n",
    "\n",
    "N__ = N_cstar(c_starr, N)\n",
    "\n",
    "def interpolate_N_c(N__,N):\n",
    "    # Create arrays for non-zero N[c] values\n",
    "    x = np.array([c for c, count in N__.items() if count > 0]).reshape(-1, 1)\n",
    "    y = np.array([count for c, count in N__.items() if count > 0])\n",
    "\n",
    "    # Create a linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(x, y)\n",
    "\n",
    "    # Predict N[c] for missing values\n",
    "    for c in range(max_c+2):\n",
    "        if c not in N__:\n",
    "            N__[c] = int(round(model.predict(np.array(c).reshape(1, -1))[0]))\n",
    "\n",
    "    return N__\n",
    "\n",
    "# Interpolate missing N[c] values\n",
    "N__ = interpolate_N_c(N__,N)\n",
    "\n",
    "# c_star corresponding to each word\n",
    "def unigram_cstar(unigram_counts, c_starr):\n",
    "    unigram_cstarr = {}\n",
    "    for word in unigram_counts:\n",
    "      if unigram_counts[word] not in c_starr:\n",
    "        c_starr[unigram_counts[word]] = unigram_counts[word] # for those c which were initially not present c_star[c] = c\n",
    "      else:\n",
    "        c_star = c_starr[unigram_counts[word]]\n",
    "        unigram_cstarr[word] = c_star\n",
    "    return unigram_cstarr\n",
    "\n",
    "result = unigram_cstar(unigram_counts, c_starr)\n",
    "\n",
    "def calculate_perplexity_uni_gt(unigram_cstar, size):\n",
    "    test_set = pd.read_csv('test_set_preprocessed.csv')\n",
    "    sentences = test_set['0']\n",
    "    perplexity_ls = []\n",
    "    perplexity_sum = 0\n",
    "    total_sentences = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        log_likelihood = 0.0\n",
    "        words = str(sentence).split()\n",
    "        n = len(words)\n",
    "        for word in words:\n",
    "            if word in unigram_cstar:\n",
    "                log_likelihood += math.log((unigram_cstar[word]) / size)\n",
    "        if log_likelihood == 0:\n",
    "          continue\n",
    "        perplexity = math.exp(-log_likelihood / n)\n",
    "        perplexity_ls.append(perplexity)\n",
    "        perplexity_sum += perplexity\n",
    "        total_sentences += 1\n",
    "\n",
    "    avg_perplexity = perplexity_sum / total_sentences\n",
    "    return avg_perplexity, perplexity_ls\n",
    "\n",
    "avg_perplexity, perplexity_ls = calculate_perplexity_uni_gt(result, size)\n",
    "print(\"Perplexity:\", avg_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BIGRAM MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Perplexity without Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average perplexity over all the sentences that are perplexable is  427.39485017002556\n",
      "the total sentences in test set are  66563\n",
      "the total no of not_perplexable sentences in the validation set are : 34794\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Here, we are calculating the perplexity of a bigram model without smoothing.\n",
    "First, we are creating a list containing all the bigram data from the corpus.\n",
    "'''\n",
    "## LIST OF ALL THE BIGRAMS\n",
    "def create_bigram_list() :\n",
    "    bigram_list=[]  ## List to store all the bigrams\n",
    "    train_set = pd.read_csv('train_set_preprocessed.csv')\n",
    "    comm = train_set['0'] ## List of all the comments in the train_set_processed.csv file\n",
    "    for comment in comm :\n",
    "        words=(str(comment).split())\n",
    "        for i in range(len(words)) :\n",
    "            if i==0 :\n",
    "                bigram_list.append(('<s>',words[i]))  ## The ('<s>', words[i]) adds the first word into the bigram_list list\n",
    "            elif i==len(words) - 1 :\n",
    "                bigram_list.append((words[i],'</s>'))  ## This line of code adds the last word into the bigrams_list list\n",
    "            if (i != len(words) - 1):\n",
    "                bigram_list.append((words[i], words[i+1]))\n",
    "        size=len(bigram_list)\n",
    "    return bigram_list, size\n",
    "\n",
    "## COUNT OF ALL UNIQUE BIGRAMS\n",
    "'''\n",
    "The count_bigrams function returns a dictionary in which all the unique bigrams and their frequency in the corpus is stored\n",
    "'''\n",
    "def count_bigrams(bigrams_list) :\n",
    "    bigrams={}\n",
    "    for bigram in bigrams_list :\n",
    "        if bigram not in bigrams :\n",
    "            bigrams[bigram]=1\n",
    "        else :\n",
    "            bigrams[bigram]+=1\n",
    "    return bigrams\n",
    "\n",
    "## FUNCTION TO CALCULATE THE PERPLEXITY WITHOUT SMOOTHING\n",
    "def calculate_perplexity_bigram(bigrams_count, unigram_counts) :\n",
    "\n",
    "    test_set=pd.read_csv('test_set_preprocessed.csv')\n",
    "    sentences=test_set['0']\n",
    "    total_sentences=len(sentences)\n",
    "    perplexity=[]\n",
    "    not_perplexable=[]\n",
    "\n",
    "    for sentence in sentences :\n",
    "\n",
    "        words=(str(sentence).split())\n",
    "        probability=1\n",
    "        words_in_sent=len(words)\n",
    "\n",
    "        for i in range(len(words)-1) :\n",
    "            if (i==0) and (('<s>',words[i]) in bigrams_count):\n",
    "                probability*=(bigrams_count[('<s>',words[i])])/total_sentences  ## The denominator will be all sentences as start of sentence occurs in every sentence\n",
    "                if (words[i] in unigram_counts) and ((words[i],words[i+1]) in bigrams_count) :\n",
    "                    probability*=(bigrams_count[(words[i], words[i+1])])/unigram_counts[words[i]]\n",
    "            elif (i==len(words)-2) and ((words[i+1], '</s>') in bigrams_count) :\n",
    "                probability*=(bigrams_count[(words[i+1],'</s>')])/total_sentences  ## The denominator will be all sentences as end of sentence occurs in every sentence\n",
    "                if words[i] in unigram_counts and ((words[i],words[i+1]) in bigrams_count) :\n",
    "                    probability *= (bigrams_count[(words[i], words[i+1])])/unigram_counts[words[i]]\n",
    "            elif (words[i] in unigram_counts) and ((words[i],words[i+1]) in bigrams_count) :\n",
    "                probability*=(bigrams_count[(words[i], words[i+1])])/unigram_counts[words[i]]\n",
    "            else :\n",
    "                probability=0  ## Value of probability is set to 0 if it doesnot occur in bigrams_list or unigrams list\n",
    "\n",
    "        if probability!=0 :\n",
    "            perplex=(1/(probability))**(1/(words_in_sent))\n",
    "            if perplex>= 1000000:\n",
    "                not_perplexable.append(sentence)\n",
    "            else :\n",
    "                perplexity.append(perplex)\n",
    "        else :\n",
    "            not_perplexable.append(sentence)\n",
    "\n",
    "    average_perplexity=sum(perplexity)/len(perplexity)  ## Average Perplexity over all the sentences in the test set\n",
    "    return average_perplexity, not_perplexable, perplexity\n",
    "\n",
    "## Calling of the functions to get the average perplexity\n",
    "bigram_list, size_of_bigrams= create_bigram_list()\n",
    "bigrams_count=count_bigrams(bigram_list)\n",
    "average_perplexity, not_perplexable, perplexity=calculate_perplexity_bigram(bigrams_count, unigram_counts)\n",
    "\n",
    "print(\"the average perplexity over all the sentences that are perplexable is \", average_perplexity)\n",
    "print(\"the total sentences in test set are \", len(perplexity)+ len(not_perplexable))\n",
    "print(\"the total no of not_perplexable sentences in the validation set are :\", len(not_perplexable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Perplexity after Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average perplexity over all the sentences that are perplexable is  341206.1201867486\n",
      "the total sentences in test set are  66563\n",
      "the total no of not_perplexable sentences in the validation set are : 230\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Laplace Smoothing helps to consider the probability of those bigrams which are not present in the bigrams_counts.\n",
    "This is achieved by adding 1 to the numerator and vocab_size to the denominator.\n",
    "In the bigram model, the probability of a sentence is the product of the probabilities of all the bigrams.\n",
    "Those bigrams which are not present in the dictionary created by the function bigrams_count are assigned the value 0 because of their 0 occurrence.\n",
    "'''\n",
    "def calculate_perplexity_bigram_smoothing(bigrams_count, unigram_counts,vocab_size) :\n",
    "    test_set=pd.read_csv('test_set_preprocessed.csv')\n",
    "    sentences=test_set['0']\n",
    "    total_sentences=len(sentences)\n",
    "    perplexity=[]\n",
    "    not_perplexable=[]\n",
    "    for sentence in sentences :\n",
    "        \n",
    "        words=str(sentence).split()\n",
    "        probability=1\n",
    "        perplex=1\n",
    "        words_in_sent=len(words)\n",
    "        \n",
    "        if (words_in_sent < 1) :\n",
    "            not_perplexable.append(sentence)\n",
    "            continue\n",
    "            \n",
    "        for i in range(len(words)-1) :\n",
    "            \n",
    "            if (i==0) :\n",
    "                ## as the denominator will be all sentences as start of sentence occurs in every sentence\n",
    "                if ('<s>', words[i]) not in bigrams_count :\n",
    "                    bigrams_count[('<s>', words[i])]=0\n",
    "                probability*=(bigrams_count[('<s>',words[i])]+1)/(total_sentences+vocab_size)\n",
    "            elif (i==len(words)-2) :\n",
    "                if (words[i+1], '</s>') not in bigrams_count :\n",
    "                    bigrams_count[(words[i+1], '</s>')]=0\n",
    "                if (words[i+1] in unigram_counts) :\n",
    "                    probability*=(bigrams_count[(words[i+1],'</s>')]+1)/(unigram_counts[words[i+1]]+vocab_size)\n",
    "                if words[i] in unigram_counts  :\n",
    "                    if (words[i],words[i+1]) not in bigrams_count:\n",
    "                        bigrams_count[(words[i],words[i+1])]=0\n",
    "                    probability*=(bigrams_count[(words[i], words[i+1])]+1)/(unigram_counts[words[i]]+vocab_size)\n",
    "                    \n",
    "            if (words[i]) in unigram_counts  :\n",
    "                if (words[i],words[i+1]) not in bigrams_count:\n",
    "                    bigrams_count[(words[i],words[i+1])]=0            \n",
    "                probability*=(bigrams_count[(words[i], words[i+1])]+1)/(unigram_counts[words[i]]+vocab_size)\n",
    "            #else :\n",
    "             #   probability=0\n",
    "                \n",
    "        if probability!=0 :\n",
    "            perplex=(1/(probability))**(1/(words_in_sent))\n",
    "            if perplex>= 10000000000:\n",
    "                not_perplexable.append(sentence)\n",
    "            else :\n",
    "                perplexity.append(perplex)\n",
    "        else :\n",
    "            not_perplexable.append(sentence)\n",
    "            \n",
    "    average_perplexity=sum(perplexity)/len(perplexity)\n",
    "    return average_perplexity, not_perplexable, perplexity\n",
    "\n",
    "bigrams_count_perplex=count_bigrams(bigram_list)\n",
    "average_perplexity, not_perplexable, perplexity = calculate_perplexity_bigram_smoothing(bigrams_count_perplex, unigramcount_perplex,vocab_size)\n",
    "\n",
    "print(\"the average perplexity over all the sentences that are perplexable is \", average_perplexity)\n",
    "print(\"the total sentences in test set are \", len(perplexity)+ len(not_perplexable))\n",
    "print(\"the total no of not_perplexable sentences in the validation set are :\", len(not_perplexable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Perplexity after Add-k Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average perplexity over all the sentences that are perplexable is  543094.666938341\n",
      "the total sentences in test set are  66563\n",
      "the total no of not_perplexable sentences in the validation set are : 159\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity_bigram_smoothing_addk(bigrams_count, unigram_counts, vocab_size) :\n",
    "    \n",
    "    test_set=pd.read_csv('test_set_preprocessed.csv')\n",
    "    sentences=test_set['0']\n",
    "    total_sentences=len(sentences)\n",
    "    perplexity=[]\n",
    "    not_perplexable=[]\n",
    "    k=0.1\n",
    "    \n",
    "    for sentence in sentences :\n",
    "        \n",
    "        words=str(sentence).split()\n",
    "        probability=1\n",
    "        perplex=1\n",
    "        words_in_sent=len(words)\n",
    "        for i in range(len(words)-1) :\n",
    "            \n",
    "            if (i==0) :\n",
    "                if ('<s>', words[i]) not in bigrams_count :  ## Denominator will be all sentences as start of sentence occurs in every sentence\n",
    "                    bigrams_count[('<s>', words[i])]=0\n",
    "                probability*=(bigrams_count[('<s>',words[i])]+k)/(total_sentences+(vocab_size*k))\n",
    "            elif (i==len(words)-2) :\n",
    "                if (words[i+1], '</s>') not in bigrams_count :\n",
    "                    bigrams_count[(words[i+1], '</s>')]=0\n",
    "                probability*=(bigrams_count[(words[i+1],'</s>')]+k)/(total_sentences+(vocab_size*k))\n",
    "                if words[i] in unigram_counts  :\n",
    "                    if (words[i],words[i+1]) not in bigrams_count:\n",
    "                        bigrams_count[(words[i],words[i+1])]=0\n",
    "                    probability*=(bigrams_count[(words[i], words[i+1])]+k)/(unigram_counts[words[i]]+(vocab_size*k))\n",
    "                    \n",
    "            if (words[i]) in unigram_counts  :\n",
    "                if (words[i],words[i+1]) not in bigrams_count:\n",
    "                    bigrams_count[(words[i],words[i+1])]=0\n",
    "                probability*=(bigrams_count[(words[i], words[i+1])]+k)/(unigram_counts[words[i]]+(vocab_size*k))\n",
    "            else :\n",
    "                probability=0\n",
    "                \n",
    "        if probability!=0 :\n",
    "            perplex=(1/(probability))**(1/(words_in_sent))\n",
    "            if perplex>= 10000000000:\n",
    "                not_perplexable.append(sentence)                \n",
    "            else :\n",
    "                perplexity.append(perplex)\n",
    "        else :\n",
    "            not_perplexable.append(sentence)\n",
    "            \n",
    "    average_perplexity=sum(perplexity)/len(perplexity)\n",
    "    return average_perplexity, not_perplexable, perplexity\n",
    "\n",
    "bigrams_count_perplex=count_bigrams(bigram_list)\n",
    "average_perplexity, not_perplexable, perplexity = calculate_perplexity_bigram_smoothing_addk(bigrams_count_perplex, unigramcount_perplex, vocab_size)\n",
    "\n",
    "print(\"the average perplexity over all the sentences that are perplexable is \", average_perplexity)\n",
    "print(\"the total sentences in test set are \", len(perplexity)+ len(not_perplexable))\n",
    "print(\"the total no of not_perplexable sentences in the validation set are :\", len(not_perplexable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Perplexity after Applying Bigram Smoothing Unigram Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average perplexity over all the sentences that are perplexable is  2550480.8580540624\n",
      "the total sentences in test set are  66563\n",
      "the total no of not_perplexable sentences in the validation set are : 133\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity_bigram_smoothing_unigram_prior(bigrams_count, unigram_counts, vocab_size) :\n",
    "    \n",
    "    test_set=pd.read_csv('test_set_preprocessed.csv')\n",
    "    sentences=test_set['0']\n",
    "    total_sentences=len(sentences)\n",
    "    perplexity=[]\n",
    "    not_perplexable=[]\n",
    "    not_perplex=[]\n",
    "    m=100\n",
    "    \n",
    "    for sentence in sentences :\n",
    "        \n",
    "        words=str(sentence).split()\n",
    "        probability=1\n",
    "        perplex=1\n",
    "        words_in_sent=len(words)\n",
    "        \n",
    "        for i in range(len(words)-1) :\n",
    "            \n",
    "            if (i==0) :\n",
    "                if ('<s>', words[i]) not in bigrams_count :\n",
    "                    bigrams_count[('<s>', words[i])]=0\n",
    "                probability*=(bigrams_count[('<s>',words[i])]+(m*((unigram_counts[words[i]])+1)/(size+vocab_size)))/(total_sentences+m)\n",
    "            elif (i==len(words)-2) :\n",
    "                if (words[i+1], '</s>') not in bigrams_count :\n",
    "                    bigrams_count[(words[i+1], '</s>')]=0\n",
    "                probability*=(bigrams_count[(words[i+1],'</s>')]+(m*((total_sentences)+1)/(size+vocab_size)))/(total_sentences+m)\n",
    "                if words[i] in unigram_counts  :\n",
    "                    if (words[i],words[i+1]) not in bigrams_count:\n",
    "                        bigrams_count[(words[i],words[i+1])]=0\n",
    "                    probability*=(bigrams_count[(words[i], words[i+1])]+(m*((unigram_counts[words[i+1]])+1)/(size+vocab_size)))/(unigram_counts[words[i]]+m)\n",
    "                    \n",
    "            if ((i != len(words) - 1) and ((words[i]) in unigram_counts))  :\n",
    "                if (words[i],words[i+1]) not in bigrams_count:\n",
    "                    bigrams_count[(words[i],words[i+1])]=0\n",
    "                probability*=(bigrams_count[(words[i], words[i+1])]+(m*((unigram_counts[words[i+1]])+1)/(size+vocab_size)))/(unigram_counts[words[i]]+m)\n",
    "            else :\n",
    "                probability=0\n",
    "                \n",
    "        if probability!=0 :\n",
    "            perplex=(1/(probability))**(1/(words_in_sent))\n",
    "            if perplex>= 10000000000:\n",
    "                not_perplexable.append(sentence)\n",
    "                not_perplex.append((perplex,probability))\n",
    "            else :\n",
    "                perplexity.append(perplex)\n",
    "        else :\n",
    "            not_perplexable.append(sentence)\n",
    "            not_perplex.append((perplex,probability))\n",
    "            \n",
    "    average_perplexity=sum(perplexity)/len(perplexity)\n",
    "    return average_perplexity, not_perplexable, perplexity, not_perplex\n",
    "\n",
    "bigrams_count_perplex=count_bigrams(bigram_list)\n",
    "average_perplexity, not_perplexable, perplexity, not_perplex_bigram=calculate_perplexity_bigram_smoothing_unigram_prior(bigrams_count_perplex, unigramcount_perplex, vocab_size)\n",
    "\n",
    "print(\"the average perplexity over all the sentences that are perplexable is \", average_perplexity)\n",
    "print(\"the total sentences in test set are \", len(perplexity)+ len(not_perplexable))\n",
    "print(\"the total no of not_perplexable sentences in the validation set are :\", len(not_perplexable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Perplexity after Applying Good Turing Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 57630.999726616195\n"
     ]
    }
   ],
   "source": [
    "def bigram_vocabulary() :\n",
    "    # list of bigrams from train set\n",
    "    train_set = pd.read_csv('train_set_preprocessed.csv')\n",
    "    comm = train_set['0']\n",
    "    bigrams_ls_train = []\n",
    "    for comment in comm :\n",
    "        words = (str(comment).split())\n",
    "        if len(words) < 1:\n",
    "          continue\n",
    "        for i in range(len(words)) :\n",
    "            if i==0 :\n",
    "                ## adding start of the sentence\n",
    "                bigrams_ls_train.append(('<s>',words[i]))\n",
    "            elif i==len(words)-1 :\n",
    "                ## adding the end of the sentence\n",
    "                bigrams_ls_train.append((words[i],'</s>'))\n",
    "            else :\n",
    "                bigrams_ls_train.append((words[i-1], words[i]))\n",
    "    size_bigram_train = len(bigrams_ls_train)\n",
    "\n",
    "    # list of bigrams from test set\n",
    "    test_set = pd.read_csv('test_set_preprocessed.csv')\n",
    "    comm = test_set['0']\n",
    "    bigrams_ls_test = []\n",
    "    for comment in comm :\n",
    "        words = (str(comment).split())\n",
    "        if len(words) < 1:\n",
    "          continue\n",
    "        for i in range(len(words)) :\n",
    "            if i==0 :\n",
    "                ## adding start of the sentence\n",
    "                bigrams_ls_test.append(('<s>',words[i]))\n",
    "            elif i==len(words)-1 :\n",
    "                ## adding the end of the sentence\n",
    "                bigrams_ls_test.append((words[i],'</s>'))\n",
    "            else :\n",
    "                bigrams_ls_test.append((words[i-1], words[i]))\n",
    "        size_bigram_test = len(bigrams_ls_test)\n",
    "\n",
    "    bigram_vocab = bigrams_ls_test + bigrams_ls_train\n",
    "    size_bigram_vocab = len(set(bigram_vocab))\n",
    "\n",
    "    return bigram_vocab, size_bigram_vocab, bigrams_ls_train, size_bigram_train\n",
    "\n",
    "bigram_vocab, size_bigram_vocab, bigrams_ls_train, size_bigram_train = bigram_vocabulary()\n",
    "\n",
    "def count_bigrams(bigrams_ls_train) :\n",
    "    bigrams_cnt = {}\n",
    "    for bigram in bigrams_ls_train :\n",
    "        if bigram not in bigrams_cnt :\n",
    "            bigrams_cnt[bigram] = 1\n",
    "        else :\n",
    "            bigrams_cnt[bigram] += 1\n",
    "    size_bigrams_cnt = len(bigrams_cnt)\n",
    "    return bigrams_cnt, size_bigrams_cnt\n",
    "\n",
    "bigrams_cnt, size_bigrams_cnt = count_bigrams(bigrams_ls_train)\n",
    "\n",
    "# N_c-\n",
    "def N_cfunc_bigrams(bigram_counts, bigram_vocab):\n",
    "    max_c = 0\n",
    "    N = {}\n",
    "    N[0] = 0\n",
    "    for bigram in bigram_vocab:\n",
    "        if bigram in bigram_counts:\n",
    "            if bigram_counts[bigram] not in N:\n",
    "                N[bigram_counts[bigram]] = 1\n",
    "            else:\n",
    "                N[bigram_counts[bigram]] += 1\n",
    "            max_c = max(bigram_counts[bigram], max_c)\n",
    "        else:\n",
    "            N[0] += 1\n",
    "\n",
    "    return N, max_c\n",
    "\n",
    "N, max_c = N_cfunc_bigrams(bigrams_cnt, bigram_vocab)\n",
    "\n",
    "# c_star corresponding to c\n",
    "def c_star_(bigrams_cnt, N, max_c):\n",
    "    c_starr = {}\n",
    "\n",
    "    for c in range(max_c + 1):\n",
    "        if c in N and N[c] != 0 and (c + 1) in N and N[c + 1] != 0:\n",
    "            c_star = ((c + 1) * N[c + 1]) / N[c]\n",
    "            c_starr[c] = c_star\n",
    "\n",
    "    return c_starr\n",
    "\n",
    "c_starr = c_star_(bigrams_cnt, N, max_c)\n",
    "\n",
    "# N corresponding to c_star\n",
    "def N_cstar(c_starr, N):\n",
    "  N__ = {}\n",
    "  for c in N:\n",
    "    if c in c_starr:\n",
    "      N__[c_starr[c]] = N[c]\n",
    "  return N__\n",
    "\n",
    "N__ = N_cstar(c_starr, N)\n",
    "\n",
    "def interpolate_N_c(N__,N):\n",
    "    # Create arrays for non-zero N[c] values\n",
    "    x = np.array([c for c, count in N__.items() if count > 0]).reshape(-1, 1)\n",
    "    y = np.array([count for c, count in N__.items() if count > 0])\n",
    "\n",
    "    # Create a linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(x, y)\n",
    "\n",
    "    # Predict N[c] for missing values\n",
    "    for c in range(max_c+2):\n",
    "        if c not in N__:\n",
    "            N__[c] = int(round(model.predict(np.array(c).reshape(1, -1))[0]))\n",
    "\n",
    "    return N__\n",
    "\n",
    "# Interpolate missing N[c] values\n",
    "N__ = interpolate_N_c(N__,N)\n",
    "\n",
    "# c_star corresponding to each word\n",
    "def bigram_cstar(bigram_counts, c_starr):\n",
    "    bigram_cstar = {}\n",
    "    for word in bigram_counts:\n",
    "      if bigram_counts[word] not in c_starr:\n",
    "        c_starr[bigram_counts[word]] = bigram_counts[word] # for those c which were initially not present c_star[c] = c\n",
    "      c_star = c_starr[bigram_counts[word]]\n",
    "      bigram_cstar[word] = c_star\n",
    "    return bigram_cstar\n",
    "\n",
    "c_star_bigram = bigram_cstar(bigrams_cnt, c_starr)\n",
    "\n",
    "def calculate_perplexity_bi_gt(c_star_bigram, size_bigram_train):\n",
    "    test_set = pd.read_csv('test_set_preprocessed.csv')\n",
    "    sentences = test_set['0']\n",
    "    perplexity_ls = []\n",
    "    perplexity_sum = 0\n",
    "    total_sentences = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        log_likelihood = 0.0\n",
    "        words = str(sentence).split()\n",
    "\n",
    "        # list of bigrams in a sentence\n",
    "        bigram_words = []\n",
    "        if len(words) < 1:\n",
    "            continue\n",
    "        for i in range(len(words)):\n",
    "            if i == 0:\n",
    "                ## adding start of the sentence\n",
    "                bigram_words.append(('<s>', words[i]))\n",
    "            elif i == len(words) - 1:\n",
    "                ## adding the end of the sentence\n",
    "                bigram_words.append((words[i], '</s>'))\n",
    "            else:\n",
    "                bigram_words.append((words[i - 1], words[i]))\n",
    "        n = len(bigram_words)\n",
    "\n",
    "        # calculate probability\n",
    "        for bigram in bigram_words:\n",
    "            if bigram in c_star_bigram:\n",
    "                log_likelihood += math.log((c_star_bigram[bigram]) / size_bigram_train)\n",
    "\n",
    "        # calculate perplexity of sentence\n",
    "        if log_likelihood != 0:\n",
    "            perplexity = math.exp(-log_likelihood / n)\n",
    "            perplexity_ls.append(perplexity)\n",
    "            perplexity_sum += perplexity\n",
    "            total_sentences += 1\n",
    "\n",
    "    avg_perplexity = perplexity_sum / total_sentences if total_sentences > 0 else 0\n",
    "    return avg_perplexity, perplexity_ls\n",
    "\n",
    "avg_perplexity, perplexity_ls = calculate_perplexity_bi_gt(c_star_bigram, size_bigram_train)\n",
    "print(\"Perplexity:\", avg_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRIGRAM MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Perplexity without Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average perplexity over all the sentences that are perplexable is  10.409408487339345\n",
      "the total sentences in test set are  66563\n",
      "the total no of not_perplexable sentences in the validation set are : 1337\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Here, we are calculating the perplexity of a trigram model without smoothing.\n",
    "First, we are creating a list containing all the trigram data from the corpus.\n",
    "'''\n",
    "## LIST OF ALL THE TRIGRAMS\n",
    "def create_trigram_list() :\n",
    "    trigram_list=[]\n",
    "    train_set=pd.read_csv('train_set_preprocessed.csv')\n",
    "    comm=train_set['0']\n",
    "    for comment in comm :\n",
    "        words=(str(comment).split())\n",
    "        words=[word.lower() if word.isalpha() else word for word in words]\n",
    "        \n",
    "        for i in range(len(words)) :\n",
    "            if i==0 :\n",
    "                ## adding start of the sentence\n",
    "                trigram_list.append(('<s>','<s>',words[i]))\n",
    "            if i==1 :\n",
    "                trigram_list.append(('<s>',words[0],words[1]))\n",
    "            if i==len(words)-2 :\n",
    "                trigram_list.append((words[i],words[i+1],'</s>'))\n",
    "            if i==len(words)-1 :\n",
    "                ## adding the end of the sentence \n",
    "                trigram_list.append((words[i],'</s>','</s>'))\n",
    "            if (i!=(len(words)-1) and i!=(len(words)-2)) :\n",
    "                trigram_list.append((words[i], words[i+1], words[i+2]))\n",
    "\n",
    "        size=len(trigram_list)\n",
    "    return trigram_list, size\n",
    "\n",
    "trigram_list, trigram_size= create_trigram_list()\n",
    "\n",
    "## COUNT OF ALL UNIQUE TRIGRAMS\n",
    "'''\n",
    "The count_trigram function returns a dictionary in which all the unique trigrams and their frequency in the corpus is stored\n",
    "'''\n",
    "def count_trigram(trigram_list) :\n",
    "    trigrams={}\n",
    "    for trigram in trigram_list :\n",
    "        if trigram not in trigrams :\n",
    "            trigrams[trigram]=1\n",
    "        else :\n",
    "            trigrams[trigram]+=1\n",
    "    return trigrams\n",
    "\n",
    "trigrams_count=count_trigram(trigram_list)\n",
    "\n",
    "## FUNCTION TO CALCULATE THE PERPLEXITY WITHOUT SMOOTHING\n",
    "def calculate_perplexity_trigram(trigrams_count,bigrams_count) :\n",
    "    \n",
    "    test_set=pd.read_csv('test_set_preprocessed.csv')\n",
    "    sentences=test_set['0']\n",
    "    total_sentences=len(sentences)\n",
    "    perplexity=[]\n",
    "    not_perplexable=[]\n",
    "\n",
    "    for sentence in sentences :\n",
    "        \n",
    "        words=(str(sentence).split())\n",
    "        words=[word.lower() if word.isalpha() else word for word in words]\n",
    "        probability=1\n",
    "        words_in_sent=len(words)\n",
    "        a=True\n",
    "        \n",
    "        for i in range(len(words)-2) :\n",
    "            \n",
    "            if (i==0) and (('<s>','<s>',words[i]) in trigrams_count):\n",
    "                ## as the denominator will be all sentences as start of sentence occurs in every sentence\n",
    "                probability*=(trigrams_count[('<s>','<s>',words[i])])/total_sentences\n",
    "                a=False\n",
    "                \n",
    "            if (i==1) and (('<s>',words[0],words[1]) in trigrams_count) and (('<s>', words[0]) in bigrams_count):\n",
    "                probability*=(trigrams_count[('<s>',words[0],words[1])])/bigrams_count[('<s>',words[0])]\n",
    "                a=False\n",
    "\n",
    "            if (i==len(words)-3) and ((words[i+1],words[i+2], '</s>') in trigrams_count) and ((words[i+2],'</s>','</s>') in trigrams_count) :\n",
    "                ## asame logic as above for denominator \n",
    "                if (words[i+1],words[i+2]) in bigrams_count :\n",
    "                    probability*=(trigrams_count[(words[i+1],words[i+2],'</s>')])/bigrams_count[(words[i+1],words[i+2])]\n",
    "                if (words[i+2],'</s>') in bigrams_count :\n",
    "                     probability*=(trigrams_count[(words[i+2],'</s>','</s>')])/bigrams_count[(words[i+2], '</s>')]\n",
    "                a=False\n",
    "\n",
    "            if ((words[i],words[i+1]) in bigrams_count) and ((words[i],words[i+1],words[i+2]) in trigrams_count) :\n",
    "                probability*=(trigrams_count[(words[i], words[i+1], words[i+2])])/bigrams_count[(words[i], words[i+1])]\n",
    "                a=False\n",
    "            \n",
    "            if a==True :\n",
    "                probability=0\n",
    "\n",
    "        if probability!=0 :\n",
    "            perplex=(1/(probability))**(1/(words_in_sent))\n",
    "            if perplex>= 1000000:\n",
    "                not_perplexable.append(sentence)\n",
    "            else :\n",
    "                perplexity.append(perplex)\n",
    "        else :\n",
    "            not_perplexable.append(sentence)\n",
    "            \n",
    "    average_perplexity=sum(perplexity)/len(perplexity)\n",
    "    return average_perplexity, not_perplexable, perplexity\n",
    "\n",
    "average_perplexity, not_perplexable, perplexity=calculate_perplexity_trigram(trigrams_count, bigrams_count)\n",
    "\n",
    "print(\"the average perplexity over all the sentences that are perplexable is \", average_perplexity)\n",
    "print(\"the total sentences in test set are \", len(perplexity)+ len(not_perplexable))\n",
    "print(\"the total no of not_perplexable sentences in the validation set are :\", len(not_perplexable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Perplexity after Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average perplexity over all the sentences that are perplexable is  195924.52785734882\n",
      "the total sentences in test set are  66563\n",
      "the total no of not_perplexable sentences in the validation set are : 436\n"
     ]
    }
   ],
   "source": [
    "## FUNCTION FOR CALCULATING PERPLEXITY FOR TRIGRAM MODEL AFTER LAPLACE SMOOTHING\n",
    "'''\n",
    "Laplace Smoothing helps to consider the probability of those trigrams which are not present in the trigrams_counts.\n",
    "This is achieved by adding 1 to the numerator and vocab_size to the denominator.\n",
    "In the trigram model, the probability of a sentence is the product of the probabilities of all the trigrams.\n",
    "Those trigrams which are not present in the dictionary created by the function trigrams_count are assigned the value 0 because of their 0 occurrence.\n",
    "'''\n",
    "def calculate_perplexity_trigram_smoothing(trigrams_count,bigrams_count, vocab_size) :\n",
    "    \n",
    "    test_set=pd.read_csv('test_set_preprocessed.csv')\n",
    "    sentences=test_set['0']\n",
    "    total_sentences=len(sentences)\n",
    "    perplexity=[]\n",
    "    not_perplexable=[]\n",
    "    \n",
    "    for sentence in sentences :\n",
    "        \n",
    "        words=(str(sentence).split())\n",
    "        probability=1\n",
    "        perplex=1\n",
    "        words_in_sent=len(words)\n",
    "        \n",
    "        for i in range(len(words)-2) :\n",
    "            a = True\n",
    "            if (i==0) :\n",
    "                if ('<s>','<s>',words[i]) not in trigrams_count:\n",
    "                    trigrams_count[('<s>','<s>',words[i])]=0\n",
    "                probability*=(trigrams_count[('<s>','<s>',words[i])]+1)/(total_sentences+vocab_size)\n",
    "                a=False\n",
    "\n",
    "            if (i==1) and (('<s>', words[0]) in bigrams_count):\n",
    "              if  ('<s>',words[0],words[1]) not in trigrams_count :\n",
    "                trigrams_count[('<s>',words[0],words[1])]=0\n",
    "              probability*=(trigrams_count[('<s>',words[0],words[1])]+1)/(bigrams_count[('<s>',words[0])]+vocab_size)\n",
    "              a=False\n",
    "\n",
    "            if ((words[i],words[i+1]) in bigrams_count) :\n",
    "                if (words[i],words[i+1],words[i+2]) not in trigrams_count:\n",
    "                    trigrams_count[(words[i],words[i+1],words[i+2])]=0\n",
    "                probability*=(trigrams_count[(words[i], words[i+1], words[i+2])]+1)/(bigrams_count[(words[i], words[i+1])]+vocab_size)\n",
    "                a=False\n",
    "                \n",
    "            if (i==len(words)-3)  :\n",
    "                if (words[i+1],words[i+2]) in bigrams_count :  ## Same logic as above for denominator \n",
    "                    if (words[i+1],words[i+2], '</s>') not in trigrams_count :\n",
    "                        trigrams_count[(words[i+1],words[i+2], '</s>')]=0\n",
    "                    probability*=(trigrams_count[(words[i+1],words[i+2],'</s>')]+1)/(bigrams_count[(words[i+1],words[i+2])]+vocab_size)\n",
    "                if (words[i+2],'</s>') in bigrams_count :\n",
    "                     if (words[i+2],'</s>','</s>') not in trigrams_count :\n",
    "                         trigrams_count[(words[i+2],'</s>','</s>')]=0\n",
    "                     probability*=(trigrams_count[(words[i+2],'</s>','</s>')]+1)/(bigrams_count[(words[i+2], '</s>')]+vocab_size)\n",
    "                a=False\n",
    "                \n",
    "            if a==True :\n",
    "                probability=0\n",
    "\n",
    "        if probability!=0 :\n",
    "            perplex=(1/(probability))**(1/(words_in_sent))\n",
    "            if perplex >= 10000000000:\n",
    "                not_perplexable.append(sentence)\n",
    "            else :\n",
    "                perplexity.append(perplex)\n",
    "        else :\n",
    "            not_perplexable.append(sentence)\n",
    "            \n",
    "    average_perplexity=sum(perplexity)/len(perplexity)\n",
    "    return average_perplexity, not_perplexable, perplexity\n",
    "\n",
    "average_perplexity, not_perplexable, perplexity = calculate_perplexity_trigram_smoothing(trigrams_count, bigrams_count_perplex, vocab_size)\n",
    "\n",
    "print(\"the average perplexity over all the sentences that are perplexable is \", average_perplexity)\n",
    "print(\"the total sentences in test set are \", len(perplexity)+ len(not_perplexable))\n",
    "print(\"the total no of not_perplexable sentences in the validation set are :\", len(not_perplexable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Perplexity after Add-k Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average perplexity over all the sentences that are perplexable is  96234.91310276849\n",
      "the total sentences in test set are  66563\n",
      "the total no of not_perplexable sentences in the validation set are : 329\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity_trigram_smoothing_addk(trigrams_count,bigrams_count, vocab_size) :\n",
    "    \n",
    "    test_set=pd.read_csv('test_set_preprocessed.csv')\n",
    "    sentences=test_set['0']\n",
    "    total_sentences=len(sentences)\n",
    "    perplexity=[]\n",
    "    not_perplexable=[]\n",
    "    k=0.1\n",
    "    \n",
    "    for sentence in sentences :\n",
    "        \n",
    "        words=(str(sentence).split())\n",
    "        probability=1\n",
    "        perplex=1\n",
    "        words_in_sent=len(words)\n",
    "        \n",
    "        for i in range(len(words)-2) :\n",
    "            a = True\n",
    "            if (i==0) :\n",
    "                if ('<s>','<s>',words[i]) not in trigrams_count:\n",
    "                    trigrams_count[('<s>','<s>',words[i])]=0\n",
    "                probability*=(trigrams_count[('<s>','<s>',words[i])]+k)/(total_sentences+(vocab_size*k))\n",
    "                a=False\n",
    "\n",
    "            if (i==1) and (('<s>', words[0]) in bigrams_count):\n",
    "              if  ('<s>',words[0],words[1]) not in trigrams_count :\n",
    "                trigrams_count[('<s>',words[0],words[1])]=0\n",
    "              probability*=(trigrams_count[('<s>',words[0],words[1])]+k)/(bigrams_count[('<s>',words[0])]+(vocab_size*k))\n",
    "              a=False\n",
    "\n",
    "            if ((words[i],words[i+1]) in bigrams_count) :\n",
    "                if (words[i],words[i+1],words[i+2]) not in trigrams_count:\n",
    "                    trigrams_count[(words[i],words[i+1],words[i+2])]=0\n",
    "                probability*=(trigrams_count[(words[i], words[i+1], words[i+2])]+k)/(bigrams_count[(words[i], words[i+1])]+(vocab_size*k))\n",
    "                a=False\n",
    "            \n",
    "            if (i==len(words)-3)  :\n",
    "                ## asame logic as above for denominator \n",
    "                if (words[i+1],words[i+2]) in bigrams_count :\n",
    "                    if (words[i+1],words[i+2], '</s>') not in trigrams_count :\n",
    "                        trigrams_count[(words[i+1],words[i+2], '</s>')]=0\n",
    "                    probability*=(trigrams_count[(words[i+1],words[i+2],'</s>')]+k)/(bigrams_count[(words[i+1],words[i+2])]+(vocab_size*k))\n",
    "                if (words[i+2],'</s>') in bigrams_count :\n",
    "                     if (words[i+2],'</s>','</s>') not in trigrams_count :\n",
    "                         trigrams_count[(words[i+2],'</s>','</s>')]=0\n",
    "                     probability*=(trigrams_count[(words[i+2],'</s>','</s>')]+k)/(bigrams_count[(words[i+2], '</s>')]+(vocab_size*k))\n",
    "                a=False\n",
    "            \n",
    "            if a==True :\n",
    "                probability=0\n",
    "\n",
    "        if probability!=0 :\n",
    "            perplex=(1/(probability))**(1/(words_in_sent))\n",
    "            if perplex>= 10000000000:\n",
    "                not_perplexable.append(sentence)\n",
    "            else :\n",
    "                perplexity.append(perplex)\n",
    "        else :\n",
    "            not_perplexable.append(sentence)\n",
    "            \n",
    "    average_perplexity=sum(perplexity)/len(perplexity)\n",
    "    return average_perplexity, not_perplexable, perplexity\n",
    "\n",
    "average_perplexity, not_perplexable, perplexity = calculate_perplexity_trigram_smoothing_addk(trigrams_count, bigrams_count_perplex, vocab_size)\n",
    "\n",
    "print(\"the average perplexity over all the sentences that are perplexable is \", average_perplexity)\n",
    "print(\"the total sentences in test set are \", len(perplexity)+ len(not_perplexable))\n",
    "print(\"the total no of not_perplexable sentences in the validation set are :\", len(not_perplexable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Preplexity after Unigram Prior Trigram Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average perplexity over all the sentences that are perplexable is  16331.281490392577\n",
      "the total sentences in test set are  66563\n",
      "the total no of not_perplexable sentences in the validation set are : 116\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Here, m = kV\n",
    "'''\n",
    "def calculate_perplexity_trigram_smoothing_unigram_prior(trigrams_count,bigrams_count,unigram_counts, vocab_size) :\n",
    "    \n",
    "    test_set=pd.read_csv('test_set_preprocessed.csv')\n",
    "    sentences=test_set['0']\n",
    "    total_sentences=len(sentences)\n",
    "    perplexity=[]\n",
    "    not_perplexable=[]\n",
    "    m=100\n",
    "    \n",
    "    for sentence in sentences :\n",
    "        \n",
    "        words=(str(sentence).split())\n",
    "        words=[word.lower() if word.isalpha() else word for word in words]\n",
    "        probability=1\n",
    "        perplex=1\n",
    "        words_in_sent=len(words)\n",
    "        \n",
    "        for i in range(len(words)-2) :\n",
    "            a = True\n",
    "            if (i==0) :\n",
    "                if ('<s>','<s>',words[i]) not in trigrams_count:\n",
    "                    trigrams_count[('<s>','<s>',words[i])]=0\n",
    "                ## as the denominator will be all sentences as start of sentence occurs in every sentence\n",
    "                probability*=(trigrams_count[('<s>','<s>',words[i])]+(m*((unigram_counts[words[i]])+1)/(size+vocab_size)))/(total_sentences+m)\n",
    "                a=False\n",
    "\n",
    "            if (i==1) and (('<s>', words[0]) in bigrams_count):\n",
    "              if  ('<s>',words[0],words[1]) not in trigrams_count :\n",
    "                trigrams_count[('<s>',words[0],words[1])]=0\n",
    "              probability*=(trigrams_count[('<s>',words[0],words[1])]+(m*((unigram_counts[words[1]])+1)/(size+vocab_size)))/(bigrams_count[('<s>',words[0])]+m)\n",
    "              a=False\n",
    "\n",
    "            if ((words[i],words[i+1]) in bigrams_count) :\n",
    "                if (words[i],words[i+1],words[i+2]) not in trigrams_count:\n",
    "                    trigrams_count[(words[i],words[i+1],words[i+2])] = 0\n",
    "                probability*=(trigrams_count[(words[i], words[i+1], words[i+2])]+(m*((unigram_counts[words[i+2]])+1)/(size+vocab_size)))/(bigrams_count[(words[i], words[i+1])]+m)\n",
    "                a=False\n",
    "                \n",
    "            if (i==len(words)-3)  :\n",
    "                ## asame logic as above for denominator \n",
    "                if (words[i+1],words[i+2]) in bigrams_count :\n",
    "                    if (words[i+1],words[i+2], '</s>') not in trigrams_count :\n",
    "                        trigrams_count[(words[i+1],words[i+2], '</s>')]=0\n",
    "                    probability*=(trigrams_count[(words[i+1],words[i+2],'</s>')]+(m*(total_sentences+1)/(size+vocab_size)))/(bigrams_count[(words[i+1],words[i+2])]+m)\n",
    "                if (words[i+2],'</s>') in bigrams_count :\n",
    "                     if (words[i+2],'</s>','</s>') not in trigrams_count :\n",
    "                         trigrams_count[(words[i+2],'</s>','</s>')]=0\n",
    "                     probability*=(trigrams_count[(words[i+2],'</s>','</s>')]+(m*(total_sentences+1)/(size+vocab_size)))/(bigrams_count[(words[i+2], '</s>')]+m)\n",
    "                a=False\n",
    "            \n",
    "            if a==True :\n",
    "                probability=0\n",
    "\n",
    "        if probability!=0 :\n",
    "            perplex=(1/(probability))**(1/(words_in_sent))\n",
    "            if perplex>= 10000000000:\n",
    "                not_perplexable.append(sentence)\n",
    "            else :\n",
    "                perplexity.append(perplex)\n",
    "        else :\n",
    "            not_perplexable.append(sentence)\n",
    "            \n",
    "    average_perplexity=sum(perplexity)/len(perplexity)\n",
    "    return average_perplexity, not_perplexable, perplexity\n",
    "\n",
    "average_perplexity, not_perplexable, perplexity = calculate_perplexity_trigram_smoothing_unigram_prior(trigrams_count, bigrams_count_perplex,unigramcount_perplex, vocab_size)\n",
    "\n",
    "print(\"the average perplexity over all the sentences that are perplexable is \", average_perplexity)\n",
    "print(\"the total sentences in test set are \", len(perplexity)+ len(not_perplexable))\n",
    "print(\"the total no of not_perplexable sentences in the validation set are :\", len(not_perplexable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Perplexity after Applying Good Turing Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 428773.0957452862\n"
     ]
    }
   ],
   "source": [
    "def trigram_vocabulary() :\n",
    "    # list of trigrams from train set\n",
    "    train_set = pd.read_csv('train_set_preprocessed.csv')\n",
    "    comm = train_set['0']\n",
    "    trigrams_ls_train = []\n",
    "    for comment in comm :\n",
    "        words = (str(comment).split())\n",
    "        if len(words) < 2:\n",
    "          continue\n",
    "        for i in range(0,len(words)) :\n",
    "            if i==0 :\n",
    "                ## adding start of the sentence\n",
    "                trigrams_ls_train.append(('<s>',words[i], words[i+1]))\n",
    "            elif i==len(words)-1:\n",
    "                ## adding the end of the sentence\n",
    "                trigrams_ls_train.append((words[i-1], words[i], '</s>'))\n",
    "            else :\n",
    "                trigrams_ls_train.append((words[i-1], words[i], words[i+1]))\n",
    "    size_trigram_train = len(trigrams_ls_train)\n",
    "\n",
    "    # list of trigrams from test set\n",
    "    test_set = pd.read_csv('test_set_preprocessed.csv')\n",
    "    comm = test_set['0']\n",
    "    trigrams_ls_test = []\n",
    "    for comment in comm :\n",
    "        words = (str(comment).split())\n",
    "        if len(words) < 2:\n",
    "          continue\n",
    "        for i in range(1,len(words)) :\n",
    "            if i==0 :\n",
    "                ## adding start of the sentence\n",
    "                trigrams_ls_test.append(('<s>',words[i], words[i+1]))\n",
    "            elif i==len(words)-1:\n",
    "                ## adding the end of the sentence\n",
    "                trigrams_ls_test.append((words[i-1], words[i], '</s>'))\n",
    "            else :\n",
    "                trigrams_ls_test.append((words[i-1], words[i], words[i+1]))\n",
    "        size_trigram_test = len(trigrams_ls_test)\n",
    "\n",
    "    trigram_vocab = trigrams_ls_test + trigrams_ls_train\n",
    "    size_trigram_vocab = len(set(trigram_vocab))\n",
    "\n",
    "    return trigram_vocab, size_trigram_vocab, trigrams_ls_train, size_trigram_train\n",
    "\n",
    "trigram_vocab, size_trigram_vocab, trigrams_ls_train, size_trigram_train = trigram_vocabulary()\n",
    "\n",
    "def count_trigrams(trigrams_ls_train) :\n",
    "    trigrams_cnt = {}\n",
    "    for trigram in trigrams_ls_train :\n",
    "        if trigram not in trigrams_cnt :\n",
    "            trigrams_cnt[trigram] = 1\n",
    "        else :\n",
    "            trigrams_cnt[trigram] += 1\n",
    "    size_trigrams_cnt = len(trigrams_cnt)\n",
    "    return trigrams_cnt, size_trigrams_cnt\n",
    "\n",
    "trigrams_cnt, size_trigrams_cnt = count_trigrams(trigrams_ls_train)\n",
    "\n",
    "# N_c-\n",
    "def N_cfunc_trigrams(trigram_counts, trigram_vocab):\n",
    "    max_c = 0\n",
    "    N = {}\n",
    "    N[0] = 0\n",
    "    for trigram in trigram_vocab:\n",
    "        if trigram in trigram_counts:\n",
    "            if trigram_counts[trigram] not in N:\n",
    "                N[trigram_counts[trigram]] = 1\n",
    "            else:\n",
    "                N[trigram_counts[trigram]] += 1\n",
    "            max_c = max(trigram_counts[trigram], max_c)\n",
    "        else:\n",
    "            N[0] += 1\n",
    "\n",
    "    return N, max_c\n",
    "\n",
    "N, max_c = N_cfunc_trigrams(trigrams_cnt, trigram_vocab)\n",
    "\n",
    "# c_star corresponding to c\n",
    "def c_star_(trigrams_cnt, N, max_c):\n",
    "    c_starr = {}\n",
    "\n",
    "    for c in range(max_c + 1):\n",
    "        if c in N and N[c] != 0 and (c + 1) in N and N[c + 1] != 0:\n",
    "            c_star = ((c + 1) * N[c + 1]) / N[c]\n",
    "            c_starr[c] = c_star\n",
    "\n",
    "    return c_starr\n",
    "\n",
    "c_starr = c_star_(trigrams_cnt, N, max_c)\n",
    "\n",
    "# N corresponding to c_star\n",
    "def N_cstar(c_starr, N):\n",
    "  N__ = {}\n",
    "  for c in N:\n",
    "    if c in c_starr:\n",
    "      N__[c_starr[c]] = N[c]\n",
    "  return N__\n",
    "\n",
    "N__ = N_cstar(c_starr, N)\n",
    "\n",
    "def interpolate_N_c(N__,N):\n",
    "    # Create arrays for non-zero N[c] values\n",
    "    x = np.array([c for c, count in N__.items() if count > 0]).reshape(-1, 1)\n",
    "    y = np.array([count for c, count in N__.items() if count > 0])\n",
    "\n",
    "    # Create a linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(x, y)\n",
    "\n",
    "    # Predict N[c] for missing values\n",
    "    for c in range(max_c+2):\n",
    "        if c not in N__:\n",
    "            N__[c] = int(round(model.predict(np.array(c).reshape(1, -1))[0]))\n",
    "\n",
    "    return N__\n",
    "\n",
    "# Interpolate missing N[c] values\n",
    "N__ = interpolate_N_c(N__,N)\n",
    "\n",
    "# c_star corresponding to each word\n",
    "def trigram_cstar(trigram_counts, c_starr):\n",
    "    trigram_cstar = {}\n",
    "    for word in trigram_counts:\n",
    "      if trigram_counts[word] not in c_starr:\n",
    "        c_starr[trigram_counts[word]] = trigram_counts[word] # for those c which were initially not present c_star[c] = c\n",
    "      c_star = c_starr[trigram_counts[word]]\n",
    "      trigram_cstar[word] = c_star\n",
    "    return trigram_cstar\n",
    "\n",
    "c_star_trigram = trigram_cstar(trigrams_cnt, c_starr)\n",
    "\n",
    "def calculate_perplexity_tri_gt(c_star_trigram, size_trigram_train):\n",
    "    test_set = pd.read_csv('test_set_preprocessed.csv')\n",
    "    sentences = test_set['0']\n",
    "    perplexity_ls = []\n",
    "    perplexity_sum = 0\n",
    "    total_sentences = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        log_likelihood = 0.0\n",
    "        words = str(sentence).split()\n",
    "\n",
    "        # list of trigrams in a sentence\n",
    "        trigram_words = []\n",
    "        if len(words) < 2:\n",
    "            continue\n",
    "        for i in range(1,len(words)) :\n",
    "            if i==0 :\n",
    "                ## adding start of the sentence\n",
    "                trigram_words.append(('<s>',words[i], words[i+1]))\n",
    "            elif i==len(words)-1:\n",
    "                ## adding the end of the sentence\n",
    "                trigram_words.append((words[i-1], words[i], '</s>'))\n",
    "            else :\n",
    "                trigram_words.append((words[i-1], words[i], words[i+1]))\n",
    "        n = len(trigram_words)\n",
    "\n",
    "        # calculate probability\n",
    "        for trigram in trigram_words:\n",
    "            if trigram in c_star_trigram:\n",
    "                log_likelihood += math.log((c_star_trigram[trigram]) / size_trigram_train)\n",
    "\n",
    "        # calculate perplexity of sentence\n",
    "        if log_likelihood != 0:\n",
    "            perplexity = math.exp(-log_likelihood / n)\n",
    "            perplexity_ls.append(perplexity)\n",
    "            perplexity_sum += perplexity\n",
    "            total_sentences += 1\n",
    "\n",
    "    avg_perplexity = perplexity_sum / total_sentences if total_sentences > 0 else 0\n",
    "    return avg_perplexity, perplexity_ls\n",
    "\n",
    "avg_perplexity, perplexity_ls = calculate_perplexity_tri_gt(c_star_trigram, size_trigram_train)\n",
    "print(\"Perplexity:\", avg_perplexity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QUADGRAM MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Perplexity Without Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average perplexity over all the sentences that are perplexable is  5.637616496767182\n",
      "the total sentences in test set are  66563\n",
      "the total no of not_perplexable sentences in the validation set are : 44551\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Here, we are calculating the perplexity of a quadgram model without smoothing.\n",
    "First, we are creating a list containing all the quadgram data from the corpus.\n",
    "'''\n",
    "## LIST OF ALL QUADGRAMS\n",
    "def create_quadgram_list() :\n",
    "    quadgram_list=[]\n",
    "    train_set=pd.read_csv(\"train_set_preprocessed.csv\")\n",
    "    comm=train_set['0']\n",
    "    for comment in comm :\n",
    "        words=(str(comment).split())\n",
    "        if(len(words) < 3): continue\n",
    "        for i in range(len(words)) :\n",
    "            if i == 0:\n",
    "                ## adding start of the sentence\n",
    "                quadgram_list.append(('<s>','<s>','<s>',words[i]))\n",
    "            if i == 1:\n",
    "                quadgram_list.append(('<s>','<s>',words[i - 1],words[i]))\n",
    "            if i == 2:\n",
    "                quadgram_list.append(('<s>',words[i - 2],words[i - 1],words[i]))\n",
    "            if i == len(words) - 3:\n",
    "                ## adding the end of the sentence\n",
    "                quadgram_list.append((words[i],words[i+1],words[i+2],'</s>'))\n",
    "            if i == len(words) - 2:\n",
    "                quadgram_list.append((words[i],words[i+1],'</s>','</s>'))\n",
    "            if i == len(words) - 1:\n",
    "                quadgram_list.append((words[i],'</s>','</s>','</s>'))\n",
    "            if (i != (len(words) - 1) and i != (len(words) - 2) and i != (len(words) - 3)):\n",
    "                quadgram_list.append((words[i], words[i + 1],words[i + 2],words[i + 3]))\n",
    "    size=len(quadgram_list)\n",
    "    return quadgram_list, size\n",
    "\n",
    "## COUNT OF ALL UNIQUE QUADGRAMS\n",
    "'''\n",
    "The count_quadgrams function returns a dictionary in which all the unique quadgrams and their frequency in the corpus is stored\n",
    "'''\n",
    "def count_quadgrams(quadgrams_list) :\n",
    "    quadgrams={}\n",
    "    for quadgram in quadgrams_list :\n",
    "        if quadgram not in quadgrams :\n",
    "            quadgrams[quadgram] = 1\n",
    "        else :\n",
    "            quadgrams[quadgram] += 1\n",
    "    return quadgrams\n",
    "\n",
    "## FUNCTION TO CALCULATE THE PERPLEXITY WITHOUT SMOOTHING\n",
    "def calculate_quadgram_perplexity(quadgrams_count,trigrams_count) :\n",
    "    \n",
    "    test_set=pd.read_csv(\"test_set_preprocessed.csv\")\n",
    "    sentences=test_set['0']\n",
    "    total_sentences=len(sentences) # number of <s>\n",
    "    perplexity=[]\n",
    "    not_perplexable=[]\n",
    "    \n",
    "    for sentence in sentences :\n",
    "        \n",
    "        words=(str(sentence).split())\n",
    "        probability=1\n",
    "        words_in_sent=len(words)\n",
    "        if(words_in_sent < 1):\n",
    "            not_perplexable.append(sentence)\n",
    "            continue\n",
    "            \n",
    "        for i in range(len(words)-2) :\n",
    "            chk = 0\n",
    "            if ((i==0) and (('<s>','<s>','<s>',words[i]) in quadgrams_count)):\n",
    "                probability *= (quadgrams_count[('<s>','<s>','<s>',words[i])])/total_sentences\n",
    "                chk = 1\n",
    "            if ((i==1) and (('<s>','<s>',words[i - 1],words[i]) in quadgrams_count) and (('<s>','<s>',words[i - 1]) in trigrams_count)):\n",
    "                probability *= (quadgrams_count[('<s>','<s>',words[i-1],words[i])])/trigrams_count[('<s>','<s>',words[i - 1])]\n",
    "                chk = 1\n",
    "            if ((i==2) and (('<s>',words[i-2],words[i - 1],words[i]) in quadgrams_count) and (('<s>',words[i-2],words[i - 1]) in trigrams_count)):\n",
    "                probability *= (quadgrams_count[('<s>',words[i-2],words[i-1],words[i])])/trigrams_count[('<s>',words[i-2],words[i - 1])]\n",
    "                chk = 1\n",
    "            if (i==len(words)-3) and ((words[i],words[i+1],words[i+2],'</s>') in quadgrams_count and ((words[i+1],words[i+2],'</s>','</s>') in quadgrams_count) and ((words[i+2],'</s>','</s>','</s>') in quadgrams_count)):\n",
    "                if ((words[i],words[i+1],words[i+2]) in trigrams_count):\n",
    "                    probability *= (quadgrams_count[(words[i],words[i+1],words[i+2],'</s>')])/trigrams_count[(words[i],words[i+1],words[i+2])]\n",
    "                    chk = 1\n",
    "                if ((words[i+1],words[i+2],'</s>') in trigrams_count):\n",
    "                    probability *= (quadgrams_count[(words[i+1],words[i+2],'</s>','</s>')])/trigrams_count[(words[i+1],words[i+2],'</s>')]\n",
    "                    chk = 1\n",
    "                if ((words[i+2],'</s>','</s>') in trigrams_count):\n",
    "                    probability *= (quadgrams_count[(words[i+2],'</s>','</s>','</s>')])/trigrams_count[(words[i+2],'</s>','</s>')]\n",
    "                    chk = 1\n",
    "            if ((i != len(words)-3) and ((words[i],words[i+1],words[i+2]) in trigrams_count) and ((words[i],words[i+1],words[i+2],words[i+3]) in quadgrams_count)) :\n",
    "                probability *=(quadgrams_count[(words[i],words[i+1],words[i+2],words[i+3])])/trigrams_count[(words[i],words[i+1],words[i+2])]\n",
    "                chk = 1\n",
    "            if chk == 0:\n",
    "                probability=0\n",
    "        \n",
    "        if probability != 0 :\n",
    "            perplex=(1/(probability))**(1/(words_in_sent))\n",
    "            if perplex >= 1000000:\n",
    "                not_perplexable.append(sentence)\n",
    "            else :\n",
    "                perplexity.append(perplex)\n",
    "        else :\n",
    "            not_perplexable.append(sentence)\n",
    "            \n",
    "    average_perplexity=sum(perplexity)/len(perplexity)\n",
    "    return average_perplexity, not_perplexable, perplexity\n",
    "\n",
    "quadgrams_list, quadgrams_size  = create_quadgram_list()\n",
    "quadgrams = count_quadgrams(quadgrams_list)\n",
    "average_perplexity, not_perplexable, perplexity = calculate_quadgram_perplexity(quadgrams,trigrams_count)\n",
    "\n",
    "print(\"the average perplexity over all the sentences that are perplexable is \", average_perplexity)\n",
    "print(\"the total sentences in test set are \", len(perplexity) + len(not_perplexable))\n",
    "print(\"the total no of not_perplexable sentences in the validation set are :\", len(not_perplexable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Perplexity after Laplace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average perplexity over all the sentences that are perplexable is  941196.1493625918\n",
      "the total sentences in test set are  66563\n",
      "the total no of not_perplexable sentences in the validation set are : 530\n"
     ]
    }
   ],
   "source": [
    "## FUNCTION FOR CALCULATING PERPLEXITY FOR QUADGRAM MODEL AFTER LAPLACE SMOOTHING\n",
    "'''\n",
    "Laplace Smoothing helps to consider the probability of those quadgrams which are not present in the quadgrams_counts.\n",
    "This is achieved by adding 1 to the numerator and vocab_size to the denominator.\n",
    "In the quadgram model, the probability of a sentence is the product of the probabilities of all the quadgrams.\n",
    "Those quadgrams which are not present in the dictionary created by the function quadgrams_count are assigned the value 0 because of their 0 occurrence.\n",
    "'''\n",
    "def calculate_perplexity_quadgram_smoothing(quadgrams_count,trigrams_count, vocab_size) :\n",
    "    \n",
    "    test_set=pd.read_csv(\"test_set_preprocessed.csv\")\n",
    "    sentences=test_set['0']\n",
    "    total_sentences=len(sentences)\n",
    "    perplexity=[]\n",
    "    not_perplexable=[]\n",
    "    not_perplex=[]\n",
    "    \n",
    "    for sentence in sentences :\n",
    "        \n",
    "        words=(str(sentence).split())\n",
    "        probability=1\n",
    "        perplex=1\n",
    "        words_in_sent=len(words)\n",
    "        if(words_in_sent < 1):\n",
    "            not_perplexable.append(sentence)\n",
    "            continue\n",
    "            \n",
    "        for i in range(len(words)-2) :\n",
    "            chk = 0\n",
    "            if (i == 0):\n",
    "                if (('<s>','<s>','<s>',words[i]) not in quadgrams_count):\n",
    "                    quadgrams_count[('<s>','<s>','<s>',words[i])] = 0\n",
    "                probability *= (quadgrams_count[('<s>','<s>','<s>',words[i])] + 1)/(total_sentences + vocab_size)\n",
    "                chk = 1\n",
    "            if ((i==1) and (('<s>','<s>',words[i - 1]) in trigrams_count)):\n",
    "                if (('<s>','<s>',words[i - 1],words[i]) not in quadgrams_count):\n",
    "                    quadgrams_count[('<s>','<s>',words[i - 1],words[i])] = 0\n",
    "                probability *= (quadgrams_count[('<s>','<s>',words[i-1],words[i])] + 1)/(trigrams_count[('<s>','<s>',words[i - 1])] + vocab_size)\n",
    "                chk = 1\n",
    "            if ((i==2) and (('<s>',words[i-2],words[i - 1]) in trigrams_count)):\n",
    "                if (('<s>',words[i-2],words[i - 1],words[i]) not in quadgrams_count):\n",
    "                    quadgrams_count[('<s>',words[i-2],words[i - 1],words[i])] = 0\n",
    "                probability *= (quadgrams_count[('<s>',words[i-2],words[i-1],words[i])] + 1)/(trigrams_count[('<s>',words[i-2],words[i - 1])] + vocab_size)\n",
    "                chk = 1\n",
    "            if (i==len(words)-3):\n",
    "                if ((words[i],words[i+1],words[i+2],'</s>') not in quadgrams_count):\n",
    "                    quadgrams_count[(words[i],words[i+1],words[i+2],'</s>')] = 0\n",
    "                if ((words[i+1],words[i+2],'</s>','</s>') not in quadgrams_count):\n",
    "                    quadgrams_count[(words[i+1],words[i+2],'</s>','</s>')] = 0\n",
    "                if ((words[i+2],'</s>','</s>','</s>') not in quadgrams_count):\n",
    "                    quadgrams_count[(words[i+2],'</s>','</s>','</s>')] = 0\n",
    "                if ((words[i],words[i+1],words[i+2]) in trigrams_count):\n",
    "                    probability *= (quadgrams_count[(words[i],words[i+1],words[i+2],'</s>')] + 1)/(trigrams_count[(words[i],words[i+1],words[i+2])] + vocab_size)\n",
    "                    chk = 1\n",
    "                if ((words[i+1],words[i+2],'</s>') in trigrams_count):\n",
    "                    probability *= (quadgrams_count[(words[i+1],words[i+2],'</s>','</s>')] + 1)/(trigrams_count[(words[i+1],words[i+2],'</s>')] + vocab_size)\n",
    "                    chk = 1\n",
    "                if ((words[i+2],'</s>','</s>') in trigrams_count):\n",
    "                    probability *= (quadgrams_count[(words[i+2],'</s>','</s>','</s>')] + 1)/(trigrams_count[(words[i+2],'</s>','</s>')] + vocab_size)\n",
    "                    chk = 1\n",
    "            if ((i != len(words)-3) and ((words[i],words[i+1],words[i+2]) in trigrams_count)):\n",
    "                if ((words[i],words[i+1],words[i+2],words[i+3]) not in quadgrams_count):\n",
    "                    quadgrams_count[(words[i],words[i+1],words[i+2],words[i+3])] = 0\n",
    "                probability *= (quadgrams_count[(words[i],words[i+1],words[i+2],words[i+3])] + 1)/(trigrams_count[(words[i],words[i+1],words[i+2])] + vocab_size)\n",
    "                chk = 1\n",
    "            if chk == 0:\n",
    "                probability=0\n",
    "\n",
    "        if probability != 0:\n",
    "            perplex=(1/(probability))**(1/(words_in_sent))\n",
    "            if perplex >= 10000000000:\n",
    "                not_perplexable.append(sentence)\n",
    "                not_perplex.append((perplex,probability))\n",
    "            else :\n",
    "                perplexity.append(perplex)\n",
    "        else :\n",
    "            not_perplexable.append(sentence)\n",
    "            not_perplex.append((perplex,probability))\n",
    "            \n",
    "    average_perplexity=sum(perplexity)/len(perplexity)\n",
    "    return average_perplexity, not_perplexable, perplexity, not_perplex\n",
    "\n",
    "average_perplexity, not_perplexable, perplexity, not_perplex_quadgram=calculate_perplexity_quadgram_smoothing(quadgrams,trigrams_count,len(vocabulary))\n",
    "\n",
    "print(\"the average perplexity over all the sentences that are perplexable is \", average_perplexity)\n",
    "print(\"the total sentences in test set are \", len(perplexity)+ len(not_perplexable))\n",
    "print(\"the total no of not_perplexable sentences in the validation set are :\", len(not_perplexable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity after Add-k Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average perplexity over all the sentences that are perplexable is  382132.259220641\n",
      "the total sentences in test set are  66563\n",
      "the total no of not_perplexable sentences in the validation set are : 429\n"
     ]
    }
   ],
   "source": [
    "## A very low value of k has been chosen to get more accurate result\n",
    "def calculate_perplexity_quadgram_smoothing_addk(quadgrams_count,trigrams_count, vocab_size) :\n",
    "    \n",
    "    test_set=pd.read_csv(\"test_set_preprocessed.csv\")\n",
    "    sentences=test_set['0']\n",
    "    total_sentences=len(sentences)\n",
    "    perplexity=[]\n",
    "    not_perplexable=[]\n",
    "    not_perplex=[]\n",
    "    k=0.01\n",
    "    \n",
    "    for sentence in sentences :\n",
    "        \n",
    "        words=(str(sentence).split())\n",
    "        probability=1\n",
    "        perplex=1\n",
    "        words_in_sent=len(words)\n",
    "        if(words_in_sent < 1):\n",
    "            not_perplexable.append(sentence)\n",
    "            continue\n",
    "            \n",
    "        for i in range(len(words)-2) :\n",
    "            chk = 0\n",
    "            if (i == 0):\n",
    "                if (('<s>','<s>','<s>',words[i]) not in quadgrams_count):\n",
    "                    quadgrams_count[('<s>','<s>','<s>',words[i])] = 0\n",
    "                probability *= (quadgrams_count[('<s>','<s>','<s>',words[i])] + k)/(total_sentences + (vocab_size*k))\n",
    "                chk = 1\n",
    "            if ((i==1) and (('<s>','<s>',words[i - 1]) in trigrams_count)):\n",
    "                if (('<s>','<s>',words[i - 1],words[i]) not in quadgrams_count):\n",
    "                    quadgrams_count[('<s>','<s>',words[i - 1],words[i])] = 0\n",
    "                probability *= (quadgrams_count[('<s>','<s>',words[i-1],words[i])] + k)/(trigrams_count[('<s>','<s>',words[i - 1])] + (vocab_size*k))\n",
    "                chk = 1\n",
    "            if ((i==2) and (('<s>',words[i-2],words[i - 1]) in trigrams_count)):\n",
    "                if (('<s>',words[i-2],words[i - 1],words[i]) not in quadgrams_count):\n",
    "                    quadgrams_count[('<s>',words[i-2],words[i - 1],words[i])] = 0\n",
    "                probability *= (quadgrams_count[('<s>',words[i-2],words[i-1],words[i])] + k)/(trigrams_count[('<s>',words[i-2],words[i - 1])] + (vocab_size*k))\n",
    "                chk = 1\n",
    "            if (i==len(words)-3):\n",
    "                if ((words[i],words[i+1],words[i+2],'</s>') not in quadgrams_count):\n",
    "                    quadgrams_count[(words[i],words[i+1],words[i+2],'</s>')] = 0\n",
    "                if ((words[i+1],words[i+2],'</s>','</s>') not in quadgrams_count):\n",
    "                    quadgrams_count[(words[i+1],words[i+2],'</s>','</s>')] = 0\n",
    "                if ((words[i+2],'</s>','</s>','</s>') not in quadgrams_count):\n",
    "                    quadgrams_count[(words[i+2],'</s>','</s>','</s>')] = 0\n",
    "                if ((words[i],words[i+1],words[i+2]) in trigrams_count):\n",
    "                    probability *= (quadgrams_count[(words[i],words[i+1],words[i+2],'</s>')] + k)/(trigrams_count[(words[i],words[i+1],words[i+2])] + (vocab_size*k))\n",
    "                    chk = 1\n",
    "                if ((words[i+1],words[i+2],'</s>') in trigrams_count):\n",
    "                    probability *= (quadgrams_count[(words[i+1],words[i+2],'</s>','</s>')] + k)/(trigrams_count[(words[i+1],words[i+2],'</s>')] + (vocab_size*k))\n",
    "                    chk = 1\n",
    "                if ((words[i+2],'</s>','</s>') in trigrams_count):\n",
    "                    probability *= (quadgrams_count[(words[i+2],'</s>','</s>','</s>')] + k)/(trigrams_count[(words[i+2],'</s>','</s>')] + (vocab_size*k))\n",
    "                    chk = 1\n",
    "            if ((i != len(words)-3) and ((words[i],words[i+1],words[i+2]) in trigrams_count)):\n",
    "                if ((words[i],words[i+1],words[i+2],words[i+3]) not in quadgrams_count):\n",
    "                    quadgrams_count[(words[i],words[i+1],words[i+2],words[i+3])] = 0\n",
    "                probability *= (quadgrams_count[(words[i],words[i+1],words[i+2],words[i+3])] + k)/(trigrams_count[(words[i],words[i+1],words[i+2])] + (vocab_size*k))\n",
    "                chk = 1\n",
    "            if chk == 0:\n",
    "                probability=0\n",
    "\n",
    "        if probability != 0:\n",
    "            perplex=(1/(probability))**(1/(words_in_sent))\n",
    "            if perplex >= 10000000000:\n",
    "                not_perplexable.append(sentence)\n",
    "                not_perplex.append((perplex,probability))\n",
    "            else :\n",
    "                perplexity.append(perplex)\n",
    "        else :\n",
    "            not_perplexable.append(sentence)\n",
    "            not_perplex.append((perplex,probability))\n",
    "            \n",
    "    average_perplexity=sum(perplexity)/len(perplexity)\n",
    "    return average_perplexity, not_perplexable, perplexity, not_perplex\n",
    "\n",
    "average_perplexity, not_perplexable, perplexity, not_perplex_quadgram=calculate_perplexity_quadgram_smoothing_addk(quadgrams,trigrams_count,len(vocabulary))\n",
    "\n",
    "print(\"the average perplexity over all the sentences that are perplexable is \", average_perplexity)\n",
    "print(\"the total sentences in test set are \", len(perplexity)+ len(not_perplexable))\n",
    "print(\"the total no of not_perplexable sentences in the validation set are :\", len(not_perplexable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d. Perplexity after Unigram Prior Quadgram Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average perplexity over all the sentences that are perplexable is  27203.24884401948\n",
      "the total sentences in test set are  66563\n",
      "the total no of not_perplexable sentences in the validation set are : 129\n"
     ]
    }
   ],
   "source": [
    "def calculate_perplexity_quadgram_smoothing_unigram_prior(quadgrams_count,trigrams_count,unigram_counts, vocab_size) :\n",
    "    \n",
    "    test_set=pd.read_csv(\"test_set_preprocessed.csv\")\n",
    "    sentences=test_set['0']\n",
    "    total_sentences=len(sentences)\n",
    "    perplexity=[]\n",
    "    not_perplexable=[]\n",
    "    not_perplex=[]\n",
    "    m=10\n",
    "    \n",
    "    for sentence in sentences :\n",
    "        \n",
    "        words=(str(sentence).split())\n",
    "        probability=1\n",
    "        perplex=1\n",
    "        words_in_sent=len(words)\n",
    "        \n",
    "        if(words_in_sent < 1):\n",
    "            not_perplexable.append(sentence)\n",
    "            continue\n",
    "            \n",
    "        for i in range(len(words)-2) :\n",
    "            chk = 0\n",
    "            if (i == 0):\n",
    "                if (('<s>','<s>','<s>',words[i]) not in quadgrams_count):\n",
    "                    quadgrams_count[('<s>','<s>','<s>',words[i])] = 0\n",
    "                probability *= (quadgrams_count[('<s>','<s>','<s>',words[i])] + (m*((unigram_counts[words[i]])+1)/(size+vocab_size)))/(total_sentences + m)\n",
    "                chk = 1\n",
    "            if ((i==1) and (('<s>','<s>',words[i - 1]) in trigrams_count)):\n",
    "                if (('<s>','<s>',words[i - 1],words[i]) not in quadgrams_count):\n",
    "                    quadgrams_count[('<s>','<s>',words[i - 1],words[i])] = 0\n",
    "                probability *= (quadgrams_count[('<s>','<s>',words[i-1],words[i])] + (m*((unigram_counts[words[i]])+1)/(size+vocab_size)))/(trigrams_count[('<s>','<s>',words[i - 1])] + m)\n",
    "                chk = 1\n",
    "            if ((i==2) and (('<s>',words[i-2],words[i - 1]) in trigrams_count)):\n",
    "                if (('<s>',words[i-2],words[i - 1],words[i]) not in quadgrams_count):\n",
    "                    quadgrams_count[('<s>',words[i-2],words[i - 1],words[i])] = 0\n",
    "                probability *= (quadgrams_count[('<s>',words[i-2],words[i-1],words[i])] + (m*((unigram_counts[words[i]])+1)/(size+vocab_size)))/(trigrams_count[('<s>',words[i-2],words[i - 1])] + m)\n",
    "                chk = 1\n",
    "            if (i==len(words)-3):\n",
    "                if ((words[i],words[i+1],words[i+2],'</s>') not in quadgrams_count):\n",
    "                    quadgrams_count[(words[i],words[i+1],words[i+2],'</s>')] = 0\n",
    "                if ((words[i+1],words[i+2],'</s>','</s>') not in quadgrams_count):\n",
    "                    quadgrams_count[(words[i+1],words[i+2],'</s>','</s>')] = 0\n",
    "                if ((words[i+2],'</s>','</s>','</s>') not in quadgrams_count):\n",
    "                    quadgrams_count[(words[i+2],'</s>','</s>','</s>')] = 0\n",
    "                if ((words[i],words[i+1],words[i+2]) in trigrams_count):\n",
    "                    probability *= (quadgrams_count[(words[i],words[i+1],words[i+2],'</s>')] + (m*(total_sentences+1)/(size+vocab_size)))/(trigrams_count[(words[i],words[i+1],words[i+2])] + m)\n",
    "                    chk = 1\n",
    "                if ((words[i+1],words[i+2],'</s>') in trigrams_count):\n",
    "                    probability *= (quadgrams_count[(words[i+1],words[i+2],'</s>','</s>')] + (m*(total_sentences+1)/(size+vocab_size)))/(trigrams_count[(words[i+1],words[i+2],'</s>')] + m)\n",
    "                    chk = 1\n",
    "                if ((words[i+2],'</s>','</s>') in trigrams_count):\n",
    "                    probability *= (quadgrams_count[(words[i+2],'</s>','</s>','</s>')] + (m*(total_sentences+1)/(size+vocab_size)))/(trigrams_count[(words[i+2],'</s>','</s>')] + m)\n",
    "                    chk = 1\n",
    "            if ((i != len(words)-3) and ((words[i],words[i+1],words[i+2]) in trigrams_count)):\n",
    "                if ((words[i],words[i+1],words[i+2],words[i+3]) not in quadgrams_count):\n",
    "                    quadgrams_count[(words[i],words[i+1],words[i+2],words[i+3])] = 0\n",
    "                probability *= (quadgrams_count[(words[i],words[i+1],words[i+2],words[i+3])] + (m*((unigram_counts[words[i+3]])+1)/(size+vocab_size)))/(trigrams_count[(words[i],words[i+1],words[i+2])] + m)\n",
    "                chk = 1\n",
    "            if chk == 0:\n",
    "                probability=0\n",
    "\n",
    "        if probability != 0:\n",
    "            perplex=(1/(probability))**(1/(words_in_sent))\n",
    "            if perplex >= 10000000000:\n",
    "                not_perplexable.append(sentence)\n",
    "                not_perplex.append((perplex,probability))\n",
    "            else :\n",
    "                perplexity.append(perplex)\n",
    "        else :\n",
    "            not_perplexable.append(sentence)\n",
    "            not_perplex.append((perplex,probability))\n",
    "            \n",
    "    average_perplexity=sum(perplexity)/len(perplexity)\n",
    "    return average_perplexity, not_perplexable, perplexity, not_perplex\n",
    "\n",
    "average_perplexity, not_perplexable, perplexity, not_perplex_quadgram=calculate_perplexity_quadgram_smoothing_unigram_prior(quadgrams,trigrams_count,unigramcount_perplex,len(vocabulary))\n",
    "\n",
    "print(\"the average perplexity over all the sentences that are perplexable is \", average_perplexity)\n",
    "print(\"the total sentences in test set are \", len(perplexity)+ len(not_perplexable))\n",
    "print(\"the total no of not_perplexable sentences in the validation set are :\", len(not_perplexable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. Perplexity after Applying Good Turing Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 240898.75011169477\n"
     ]
    }
   ],
   "source": [
    "def quadgram_vocabulary() :\n",
    "    # list of quadgrams from train set\n",
    "    train_set = pd.read_csv('train_set_preprocessed.csv')\n",
    "    comm = train_set['0']\n",
    "    quadgrams_ls_train = []\n",
    "    for comment in comm :\n",
    "        words = (str(comment).split())\n",
    "        if len(words) < 3:\n",
    "          continue\n",
    "        for i in range(1,len(words)) :\n",
    "            if i == 1:\n",
    "                ## adding start of the sentence\n",
    "                quadgrams_ls_train.append(('<s>',words[i-1], words[i],words[i+1]))\n",
    "            elif i==len(words)-1:\n",
    "                ## adding the end of the sentence\n",
    "                quadgrams_ls_train.append((words[i-2], words[i-1], words[i], '</s>'))\n",
    "            else:\n",
    "                quadgrams_ls_train.append((words[i-2],words[i-1], words[i], words[i+1]))\n",
    "    size_quadgram_train = len(quadgrams_ls_train)\n",
    "\n",
    "    # list of quadgrams from test set\n",
    "    test_set = pd.read_csv('test_set_preprocessed.csv')\n",
    "    comm = test_set['0']\n",
    "    quadgrams_ls_test = []\n",
    "    for comment in comm :\n",
    "        words = (str(comment).split())\n",
    "        if len(words) < 3:\n",
    "          continue\n",
    "        for i in range(1,len(words)) :\n",
    "            if i == 1:\n",
    "                ## adding start of the sentence\n",
    "                quadgrams_ls_test.append(('<s>',words[i-1], words[i],words[i+1]))\n",
    "            elif i==len(words)-1:\n",
    "                ## adding the end of the sentence\n",
    "                quadgrams_ls_test.append((words[i-2], words[i-1], words[i], '</s>'))\n",
    "            else:\n",
    "                quadgrams_ls_test.append((words[i-2],words[i-1], words[i], words[i+1]))\n",
    "    size_quadgram_train = len(quadgrams_ls_test)\n",
    "\n",
    "    quadgram_vocab = quadgrams_ls_test + quadgrams_ls_train\n",
    "    size_quadgram_vocab = len(set(quadgram_vocab))\n",
    "\n",
    "    return quadgram_vocab, size_quadgram_vocab, quadgrams_ls_train, size_quadgram_train\n",
    "\n",
    "quadgram_vocab, size_quadgram_vocab, quadgrams_ls_train, size_quadgram_train = quadgram_vocabulary()\n",
    "\n",
    "def count_quadgrams(quadgrams_ls_train) :\n",
    "    quadgrams_cnt = {}\n",
    "    for quadgram in quadgrams_ls_train :\n",
    "        if quadgram not in quadgrams_cnt :\n",
    "            quadgrams_cnt[quadgram] = 1\n",
    "        else :\n",
    "            quadgrams_cnt[quadgram] += 1\n",
    "    size_quadgrams_cnt = len(quadgrams_cnt)\n",
    "    return quadgrams_cnt, size_quadgrams_cnt\n",
    "\n",
    "quadgrams_cnt, size_quadgrams_cnt = count_quadgrams(quadgrams_ls_train)\n",
    "\n",
    "# N_c-\n",
    "def N_cfunc_quadgrams(quadgram_counts, quadgram_vocab):\n",
    "    max_c = 0\n",
    "    N = {}\n",
    "    N[0] = 0\n",
    "    for quadgram in quadgram_vocab:\n",
    "        if quadgram in quadgram_counts:\n",
    "            if quadgram_counts[quadgram] not in N:\n",
    "                N[quadgram_counts[quadgram]] = 1\n",
    "            else:\n",
    "                N[quadgram_counts[quadgram]] += 1\n",
    "            max_c = max(quadgram_counts[quadgram], max_c)\n",
    "        else:\n",
    "            N[0] += 1\n",
    "\n",
    "    return N, max_c\n",
    "\n",
    "N, max_c = N_cfunc_quadgrams(quadgrams_cnt, quadgram_vocab)\n",
    "\n",
    "# c_star corresponding to c\n",
    "def c_star_(quadgrams_cnt, N, max_c):\n",
    "    c_starr = {}\n",
    "\n",
    "    for c in range(max_c + 1):\n",
    "        if c in N and N[c] != 0 and (c + 1) in N and N[c + 1] != 0:\n",
    "            c_star = ((c + 1) * N[c + 1]) / N[c]\n",
    "            c_starr[c] = c_star\n",
    "\n",
    "    return c_starr\n",
    "\n",
    "c_starr = c_star_(quadgrams_cnt, N, max_c)\n",
    "\n",
    "# N corresponding to c_star\n",
    "def N_cstar(c_starr, N):\n",
    "  N__ = {}\n",
    "  for c in N:\n",
    "    if c in c_starr:\n",
    "      N__[c_starr[c]] = N[c]\n",
    "  return N__\n",
    "\n",
    "N__ = N_cstar(c_starr, N)\n",
    "\n",
    "def interpolate_N_c(N__,N):\n",
    "    # Create arrays for non-zero N[c] values\n",
    "    x = np.array([c for c, count in N__.items() if count > 0]).reshape(-1, 1)\n",
    "    y = np.array([count for c, count in N__.items() if count > 0])\n",
    "\n",
    "    # Create a linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(x, y)\n",
    "\n",
    "    # Predict N[c] for missing values\n",
    "    for c in range(max_c+2):\n",
    "        if c not in N__:\n",
    "            N__[c] = int(round(model.predict(np.array(c).reshape(1, -1))[0]))\n",
    "\n",
    "    return N__\n",
    "\n",
    "# Interpolate missing N[c] values\n",
    "N__ = interpolate_N_c(N__,N)\n",
    "\n",
    "# c_star corresponding to each word\n",
    "def quadgram_cstar(quadgram_counts, c_starr):\n",
    "    quadgram_cstar = {}\n",
    "    for word in quadgram_counts:\n",
    "      if quadgram_counts[word] not in c_starr:\n",
    "        c_starr[quadgram_counts[word]] = quadgram_counts[word] # for those c which were initially not present c_star[c] = c\n",
    "      c_star = c_starr[quadgram_counts[word]]\n",
    "      quadgram_cstar[word] = c_star\n",
    "    return quadgram_cstar\n",
    "\n",
    "c_star_quadgram = quadgram_cstar(quadgrams_cnt, c_starr)\n",
    "\n",
    "def calculate_perplexity(c_star_quadgram, size_quadgram_train):\n",
    "    test_set = pd.read_csv('test_set_preprocessed.csv')\n",
    "    sentences = test_set['0']\n",
    "    perplexity_ls = []\n",
    "    perplexity_sum = 0\n",
    "    total_sentences = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        log_likelihood = 0.0\n",
    "        words = str(sentence).split()\n",
    "\n",
    "        # list of quadgrams in a sentence\n",
    "        quadgram_words = []\n",
    "        if len(words) < 3:\n",
    "            continue\n",
    "        for i in range(1,len(words)) :\n",
    "            if i == 1:\n",
    "                ## adding start of the sentence\n",
    "                quadgram_words.append(('<s>',words[i-1], words[i],words[i+1]))\n",
    "            elif i==len(words)-1:\n",
    "                ## adding the end of the sentence\n",
    "                quadgram_words.append((words[i-2], words[i-1], words[i], '</s>'))\n",
    "            else:\n",
    "                quadgram_words.append((words[i-2],words[i-1], words[i], words[i+1]))\n",
    "        n = len(quadgram_words)\n",
    "\n",
    "        # calculate probability\n",
    "        for quadgram in quadgram_words:\n",
    "            if quadgram in c_star_quadgram:\n",
    "                log_likelihood += math.log((c_star_quadgram[quadgram]) / size_quadgram_train)\n",
    "\n",
    "        # calculate perplexity of sentence\n",
    "        if log_likelihood != 0:\n",
    "            perplexity = math.exp(-log_likelihood / n)\n",
    "            perplexity_ls.append(perplexity)\n",
    "            perplexity_sum += perplexity\n",
    "            total_sentences += 1\n",
    "\n",
    "    avg_perplexity = perplexity_sum / total_sentences\n",
    "    return avg_perplexity, perplexity_ls\n",
    "\n",
    "avg_perplexity, perplexity_ls = calculate_perplexity(c_star_quadgram, size_quadgram_train)\n",
    "print(\"Perplexity:\", avg_perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
